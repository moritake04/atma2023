{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = pd.read_csv(\"../../data/input/anime.csv\")\n",
    "sample_submission = pd.read_csv(\"../../data/input/sample_submission.csv\")\n",
    "test = pd.read_csv(\"../../data/input/test.csv\")\n",
    "train = pd.read_csv(\"../../data/input/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "        \"genres\",\n",
    "        \"japanese_name\",\n",
    "        \"aired\",\n",
    "        \"producers\",\n",
    "        \"licensors\",\n",
    "        \"studios\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_text_only = anime[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime[\"concated\"] = anime_text_only.apply(lambda row: \" \".join(row), axis=1)\n",
    "text.append(\"concated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wakati = MeCab.Tagger(\"/opt/homebrew/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "\n",
    "def mecab_tokenizer(s: str):\n",
    "    parse_result = wakati.parse(s)\n",
    "    return [\n",
    "        result.split(\"\\t\")[0]\n",
    "        for result in parse_result.split(\"\\n\")\n",
    "        if result not in [\"EOS\", \"\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "japanese_name\n",
      "(2000, 3591)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moritake/data_science/others/atma2023/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aired\n",
      "(2000, 91)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moritake/data_science/others/atma2023/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "producers\n",
      "(2000, 746)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moritake/data_science/others/atma2023/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "licensors\n",
      "(2000, 79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moritake/data_science/others/atma2023/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studios\n",
      "(2000, 271)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moritake/data_science/others/atma2023/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concated\n",
      "(2000, 4479)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moritake/data_science/others/atma2023/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for cat in text:\n",
    "    if cat == \"genres\":\n",
    "        continue\n",
    "    print(cat)\n",
    "    tfidf = TfidfVectorizer(tokenizer=mecab_tokenizer)\n",
    "    svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "\n",
    "    feature_tfidf = tfidf.fit_transform(anime[cat])\n",
    "    print(feature_tfidf.shape)\n",
    "    feature_svd = svd.fit_transform(feature_tfidf)\n",
    "    columns = [f\"{cat}_{i}\" for i in range(50)]\n",
    "\n",
    "    anime_embs = pd.DataFrame()\n",
    "    embs_df = pd.DataFrame(feature_svd, columns=columns)\n",
    "    anime_embs[\"anime_id\"] = anime[\"anime_id\"]\n",
    "    anime_embs = pd.concat([anime_embs, embs_df], axis=1)\n",
    "\n",
    "    anime_embs.to_csv(f\"../../data/input/created_features/anime_{cat}_svd50.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genres\n",
      "(2000, 48)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moritake/data_science/others/atma2023/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for cat in text:\n",
    "    if cat != \"genres\":\n",
    "        continue\n",
    "    print(cat)\n",
    "    tfidf = TfidfVectorizer(tokenizer=mecab_tokenizer)\n",
    "\n",
    "    feature_tfidf = tfidf.fit_transform(anime[cat])\n",
    "    print(feature_tfidf.shape)\n",
    "    feature_tfidf = feature_tfidf.toarray()\n",
    "    columns = tfidf.get_feature_names_out()\n",
    "\n",
    "    # ゴミをリネーム\n",
    "    columns[0] = \"comma\"\n",
    "    columns[1] = \"hyphen\"\n",
    "\n",
    "    # ゴミを消す\n",
    "    #feature_tfidf = feature_tfidf[:, 2:]\n",
    "    #columns = columns[2:]\n",
    "\n",
    "    anime_embs = pd.DataFrame()\n",
    "    embs_df = pd.DataFrame(feature_tfidf, columns=columns)\n",
    "    anime_embs[\"anime_id\"] = anime[\"anime_id\"]\n",
    "    anime_embs = pd.concat([anime_embs, embs_df], axis=1)\n",
    "\n",
    "    anime_embs.to_csv(f\"../../data/input/created_features/anime_{cat}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
